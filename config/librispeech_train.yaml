model:
  base_learning_rate: 1.0e-4
  scale_lr: False
  target: src.model.LALM.LALMModel
  params:
    audio_encoder:
      target: src.model.audio_encoder.WhisperAudioEncoder
      params:
        model_name: "/home/jhkim/jhkim/model/whisper-large-v3"
        finetune: False
        use_lora: False
        use_qlora: True
        trust_remote_code: True
    
    llm_decoder:
      target: src.model.LLM.LLMDecoder
      params:
        model_name: "meta-llama/Llama-3.2-3B-Instruct"
        use_qlora: True
        use_lora: False
        lora_r: 16
        lora_alpha: 32
        lora_dropout: 0.05
        trust_remote_code: True
    
    projector_hidden_act: "gelu"
    projector_bias: False

data:
  target: src.data.librispeech.LibriSpeechDataset
  params:
    data_root: null
    batch_size: 4
    num_workers: 4
    train:
      target: src.data.librispeech.LibriSpeechDataset
      params:
        jsonl_file: "dataset/Librispeech/librispeech_train.jsonl"
        librispeech_root: null  # null이면 환경변수 LIBRISPEECH_ROOT 또는 기본 경로 사용
        stage: 1
    validation:
      target: src.data.librispeech.LibriSpeechDataset
      params:
        jsonl_file: "dataset/Librispeech/librispeech_dev.jsonl"
        librispeech_root: null  # null이면 환경변수 LIBRISPEECH_ROOT 또는 기본 경로 사용
        stage: 1

lightning:
  callbacks:
    model_checkpoint:
      target: pytorch_lightning.callbacks.ModelCheckpoint
      params:
        filename: "{epoch:04}-{step:06}"
        every_n_epochs: 1
        save_top_k: 3
        monitor: "val/loss"
        mode: "min"
        save_weights_only: False
  
  trainer:
    benchmark: True
    batch_size: 4
    num_workers: 4
    num_nodes: 1
    max_epochs: 100
    accelerator: "gpu"
    devices: 1
    precision: 16-mixed
    accumulate_grad_batches: 1
    gradient_clip_val: 1.0
    log_every_n_steps: 50
    val_check_interval: 0.5

