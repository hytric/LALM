# Base Trainer Config
callbacks:
  model_checkpoint:
    target: pytorch_lightning.callbacks.ModelCheckpoint
    params:
      filename: "{epoch:04}-{step:06}"
      every_n_epochs: 1
      save_top_k: 3
      monitor: "val/loss"
      mode: "min"
      save_weights_only: False

trainer:
  benchmark: True
  batch_size: 4
  num_workers: 4
  num_nodes: 1
  max_epochs: 100
  accelerator: "gpu"
  devices: 1
  precision: 16-mixed
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  log_every_n_steps: 50
  val_check_interval: 0.5
  resume_from_checkpoint: null

